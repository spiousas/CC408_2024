{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnMR3sOW6j5y"
   },
   "source": [
    "# Tutorial de Big Data\n",
    "## Tutorial 4 - Componentes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFE_dv7D75Da"
   },
   "source": [
    "Componentes principales (PCA, en inglés) es una técnica de **aprendizaje no supervisado**. Es decir que nos encontramos en una situación donde tenemos información de un conjunto de variables o features ($X_1, X_2, ..., X_p$), pero no sobre una variable de resultado o outcome ($Y$). Vamos a tratar de ajustar algoritmos que nos permitan entender la relación entre nuestros datos, trabajando con su propia naturaleza (su covarianza) y sin un outcome de interés $Y$.  \n",
    "\n",
    "Esto se diferencia del **aprendizaje supervisado**, caso en el cual los estimadores se usan para **predecir** resultados basados en datos que poseen un outcome o variable de resultado $Y$ (puede ser una etiqueta -clasificación- o un valor -regresión-). Por ejemplo, una regresión lineal o una regresión logística para el caso de clasificación.\n",
    "\n",
    "Los algoritmos de aprendizaje no supervisado pueden ser muy útiles para casos en los que se busca **reducir la dimensionalidad**, por ejemplo cuando se busca visualizar datos de gran dimensionalidad o se busca crear un índice. PCA suele emplearse como parte del **análisis descriptivo y exploratorio de datos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos $n$ observaciones y $p$ variables y queremos visualizarlas como parte de una análisis exploratorio de los datos.\n",
    "\n",
    "Podríamos realizar gráficos de a 2 variables, pero serían muchos si $p$ es grande...\n",
    "Entonces vamos a buscar una representación de los datos en menos dimensiones (2 usualmente) que capture la mayor información (varianza explicada) posible.\n",
    "\n",
    "Las dimensiones serán combinaciones lineales de las $p$ variables, resultando en direcciónes que maximizan la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWiCdrSF_fqo"
   },
   "source": [
    "Vamos a trabajar con la librería [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1 - Crímenes violentos por estado de Estados Unidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install numpy\n",
    "#!pip install scikit-learn\n",
    "#!pip install seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con una base de datos que provee el libro ISLP.\n",
    "\n",
    "##### Violent Crime Rates by US State\n",
    "Contiene información sobre arrestos por asaltos, asesinatos y violaciones cada 100.000 habitantes en 50 estados de Estados Unidos en 1973. También tiene información sobre el porcentaje de población viviendo en zonas urbanas.\n",
    "\n",
    "- Murder: Murder arrests (per 100,000)\n",
    "- Assault: Assault arrests (per 100,000)\n",
    "- Rape: Rape arrests (per 100,000)\n",
    "- UrbanPop: Percent urban population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Volumes/GoogleDrive-111652351013091046884/My Drive/Docencia/Ciencia de Datos/Tutorial 2024/Tutorial4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_data = pd.read_csv(\"USArrests.csv\")\n",
    "columns_names=arrests_data.columns.tolist()\n",
    "print(\"Columns names:\")\n",
    "print(columns_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vamos a quedar sólo con las columnas numéricas y esta será nuestra matriz **x** (de dimensiones nxp, con n=50 y p=4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests = arrests_data[[\"Murder\", \"Assault\", \"UrbanPop\", \"Rape\"]]\n",
    "print(arrests.head())\n",
    "arrests.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver también la correlación entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = arrests.corr()\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n",
    "\n",
    "plt.title('Correlation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calulemos la media y la desviación estándar (sd) de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arrests.mean())\n",
    "print(arrests.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la media no es cero y, más críticamente, la sd no es uno. ¿Cuál es el principal problema de buscar las componentes principales con datos que no están escalados (media = 0 y sd = 1)?\n",
    "\n",
    "Vamos a usar el scaler de `sklearn` (la función que importamos `StandardScaler`) para llevarlas a media 0 y sd 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos las variables\n",
    "# Inicializamos el transformador\n",
    "scaler = StandardScaler(with_std=True, with_mean=True)\n",
    "# Aplicamos fit_transform al DataFrame\n",
    "arrests_transformed = pd.DataFrame(scaler.fit_transform(arrests), columns=arrests.columns)\n",
    "print(arrests_transformed.mean()) # luego de la estandarización la media es cero\n",
    "print(arrests_transformed.std()) # la desviación estandar es uno\n",
    "display(arrests_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, vemos que la media es 0 y la sd 1.\n",
    "\n",
    "Ahora que tenemos las variables escaleadas veamos de nuevo la correlación. ¿Qué debería cambiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = arrests_transformed.corr()\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n",
    "\n",
    "plt.title('Correlation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Por defecto, PCA() centra las variables para que tengan media cero pero no las escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos PCA. Estamos buscando maximizar la varianza de los predictores con la restricción de normalización.\n",
    "\n",
    "Primero vamos a ajustar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "pca = PCA()\n",
    "arrests_pca = pca.fit_transform(arrests_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos ver la varianza explicada por cada componente, esto está en `explained_variance_ratio_` del objeto en el que hayamos ajustados los componentes principales. En nuestro caso `pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de la Varianza explicada por los componentes \n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver los _loadings_. Recordemos que estos son vectores que nos dicen cómo se proyecta cada valor de las coordenadas originales en cada una de las componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadings vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) \n",
    "pca.components_[0,0] #loadings del CP1 variable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta interesante notar que si tomamos los *loadings* del primer componente principal y los sumamos elevados al cuadrado, es decir, calculamos su norma euclidea (en criollo, el módulo):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loading_vectors[0,0])**2+(loading_vectors[0,1])**2+(loading_vectors[0,2])**2+(loading_vectors[0,3])**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta suma es igual a 1 ya que es la restricción que pusimos cuando desarrollamos el métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las coordenadas originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordenadas originales\n",
    "arrests_transformed[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también extraer los *scores*. Recordemos que los *scores* son las proyecciones de los puntos originales en este nuevo espacio de coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "scores = arrests_pca # Salen directamente del elemento arrests_pca\n",
    "scores[0:5] # Vemos los primeros 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si quisiéramos el valor de la proyección en la primera componente principal del primer ítem (o sea, el primer elemento de su vector de *scores*), debemos multiplicar cada componente original por su loading y sumar todo. De forma general:\n",
    "\n",
    "$$z_{j,m} = \\phi_{j,1} x_{j,1} + \\phi_{j,2} x_{j,2} + ... + \\phi_{j,p} x_{j,p},$$\n",
    "\n",
    "donde $z_{j,m}$ es la proyección del elemento j-ésimo en la componente principal m-ésima (*scores*), $\\phi_{j,p}$ es el elemento  *loading*\n",
    "\n",
    "Por ejemplo, si quisiéramos la proyección del primer elemento de la muestra en la componente principal 1 la cuenta que tendríamos que hacer es esta:\n",
    "\n",
    "$$z_{1,1} = \\phi_{1,1} x_{1,1} + \\phi_{1,2} x_{1,2} + \\phi_{1,3} x_{1,3} + + \\phi_{1,4} x_{1,4},$$\n",
    "\n",
    "calculémosma y comparémosla con el primer elemento de `scores[0]` (los *scores* del primer elemento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos arrests_transformed (el escaleado) a un numpy array\n",
    "arrests_array = arrests_transformed.to_numpy()\n",
    "\n",
    "# Ahora hacemos la suma de cada componente de la primera fila multiplicado con cada componente del primer loading\n",
    "print(arrests_array[0][0] * loading_vectors[0][0] + \n",
    " arrests_array[0][1] * loading_vectors[0][1] + \n",
    " arrests_array[0][2] * loading_vectors[0][2] +\n",
    " arrests_array[0][3] * loading_vectors[0][3])\n",
    "\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si quisiéramos calcular la la proyección (score) del primer item en la segunda compomente principal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queda como ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- Disgresión optativa de álgebra lineal ----\n",
    "Ahora una para los fanáticos del álgebra lineal: ¿Reconocen qué operación estamos haciendo?\n",
    "\n",
    "¡Sí, Un producto escalar entre las componentes originales y las *loadings*!\n",
    "\n",
    "Exactamente sería que:\n",
    "\n",
    "$$z_{1,1} = x_{1} \\cdot \\phi_{1}^T,$$\n",
    "\n",
    "Hagamos esa cuenta de nuevo par ala primera componente del primer elemento de la muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función dot del módulo numpy es el producto escalar (porque en inglés se llama dot product)\n",
    "# Y hacemos el producto escalar entre la primera fila de arrests_array y de loading_vectors\n",
    "np.dot(arrests_array[0], loading_vectors[0].transpose())\n",
    "\n",
    "# ¿Y la segunda?\n",
    "# Queda como ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces $z_{1,1} = x_{1} \\cdot \\phi_{1}^T$, $z_{1,2} = x_{1} \\cdot \\phi_{2}^T$, etc...\n",
    "\n",
    "Por último, debe haber más de un fanático *top tier* del álgebra lineal que ya habrá notado algo.\n",
    "\n",
    "Claro, lo que todos están pensando, que las proyecciones en las nuevas coordenadas de la primera muestra (*scores*) es el producto entre la matriz de *loadings* con el vector de la primera muestra ;). O sea:\n",
    "\n",
    "$$z_{1} = x_{1} \\times \\phi^T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul es la función de numpy para la multiplicación de matrices\n",
    "# Ahora lo que hacemos es multiplicar la matriz de loadings por el primer punto\n",
    "np.matmul(arrests_array[0], loading_vectors.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces obtenemos la primera fila de la matriz de *scores*, pero momento que hay más. \n",
    "\n",
    "Si $z_{1} = x_{1} \\times \\phi^T$, entonces la lógica indica que:\n",
    "\n",
    "$$Z = X \\times \\phi^T,$$\n",
    "\n",
    "es decir, la matriz de *scores* es el producto de matrices entre la matriz de datos y la de *loadings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_a_mano = np.matmul(arrests_array, loading_vectors.transpose())\n",
    "print(scores_a_mano[0:5,])\n",
    "print(scores[0:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- FIN disgresión optativa de álgebra lineal ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y qué pasa con la matriz de correlación de los scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = pd.DataFrame(scores).corr()\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n",
    "\n",
    "plt.title('Score correlation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ver el biplot. Recordemos que el biplot es una representación en la que podemos ver de forma conjunta los *scores* (los puntitos) y las *loadings* (las flechitas). En general se grafica para las primeras dos componentes principales aunque a veces se puede graficar para las primeras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "i, j = 0, 1 # Componentes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5)) # creamos 1 subplot\n",
    "ax.scatter(scores[:,0], scores[:,1]) # graficamos los valores de los CP1 y CP2\n",
    "ax.set_xlabel('CP%d' % (i+1))\n",
    "ax.set_ylabel('CP%d' % (j+1))\n",
    "for k in range(pca.components_.shape[1]): # loop que itera por la cantidad de features\n",
    "    ax.arrow(0, 0, pca.components_[i,k], pca.components_[j,k]) # flecha desde el origen (0) a las coordenadas\n",
    "    ax.text(pca.components_[i,k], pca.components_[j,k], arrests.columns[k]) # al final de cada flecha, nombre de la variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoremos un poquito el biplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "# Ajustes, extendemos longitud de las flechas e invertimos el eje y\n",
    "\n",
    "i, j = 0, 1 # Componentes\n",
    "\n",
    "scale_arrow = s_ = 2 # para extender la longitud de las flechas y que se vean mejor\n",
    "scores[:,1] *= -1\n",
    "pca.components_[1] *= -1 # gira el eje y (CP2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(scores[:,0], scores[:,1]) \n",
    "ax.set_xlabel('CP%d' % (i+1))\n",
    "ax.set_ylabel('CP%d' % (j+1))\n",
    "for k in range(pca.components_.shape[1]):\n",
    "    ax.arrow(0, 0, s_*pca.components_[i,k], s_*pca.components_[j,k])\n",
    "    ax.text(s_*pca.components_[i,k], s_*pca.components_[j,k], arrests.columns[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos decir sobre la relación entre las coordenadas originales y las primeras dos componentes principales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_state = pd.DataFrame(scores)\n",
    "scores_state['State'] = arrests_data['State']\n",
    "scores_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "# Ajustes, extendemos longitud de las flechas e invertimos el eje y\n",
    "\n",
    "i, j = 0, 1 # Componentes\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(scores[:,0], scores[:,1], s=2) \n",
    "ax.set_xlabel('CP%d' % (i+1))\n",
    "ax.set_ylabel('CP%d' % (j+1))\n",
    "for k in range(pca.components_.shape[1]):\n",
    "    ax.arrow(0, 0, s_*pca.components_[i,k], s_*pca.components_[j,k])\n",
    "    ax.text(s_*pca.components_[i,k], s_*pca.components_[j,k], arrests.columns[k])\n",
    "\n",
    "# Adding text labels\n",
    "for i in range(len(scores_state)):\n",
    "    ax.text(scores_state[0][i], scores_state[1][i], scores_state['State'][i], fontsize=9, ha='center', va='center', color = 'blue')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos decir de los países?\n",
    "\n",
    "Miremos ahora un dato importante, la proporción de varianza explicada por cada componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de la Varianza explicada por los componentes \n",
    "print(pca.explained_variance_ratio_) # CP1 explica el 62% de la varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto lo podemos pensar también como la varianza de cada nueva coordenada sobre la varianza total (que es 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(scores[:,0])/4)\n",
    "print(np.var(scores[:,1])/4)\n",
    "print(np.var(scores[:,2])/4)\n",
    "print(np.var(scores[:,3])/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos esto gráficamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4)) # 2 subplots uno al lado del otro\n",
    "ticks = np.arange(pca.n_components_)+1 # para crear ticks en el eje horizontal\n",
    "ax = axes[0]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_ , marker='o')\n",
    "ax.set_xlabel('Componente principal');\n",
    "ax.set_ylabel('Proporción de la varianza explicada por cada componente')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xticks(ticks)\n",
    "# capture suprime la visualización de la figura parcialmente terminada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[1]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_.cumsum(), marker='o') \n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Suma acumulada de la varianza explicada')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(ticks)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podmeos decir sobre la representación bidimensional de los datos viendo esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2 - Características organolépticas del vino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con un dataset de vinos (de scikit-learn). Contiene características de 178 vinos y a qué segmento de consumidores pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset y breve exploración\n",
    "wine_data = pd.read_csv('wine.csv')\n",
    "print(wine_data.shape)\n",
    "print(wine_data.dtypes)\n",
    "print(wine_data.head())\n",
    "print(wine_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos 178 samples en un espacio de 14 coordenadas.\n",
    "\n",
    "Estos datos los vamos a usar más adelante para aprendizaje supervisado (es decir, cuando tenemos X e Y). De momento vamos a quedar con la matriz X conformada por todas las columnas salvo `customer_segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos datos entre X e Y (por ahora, haremos de cuenta que no contamos con Y)\n",
    "wine_features = wine_data.iloc[:, 0:13].values\n",
    "wine_customer_segment = wine_data.iloc[:, 13].values\n",
    "\n",
    "# Vemos las etiquetas posibles de customer segment\n",
    "wine_customer_segment_unique, counts = np.unique(wine_customer_segment, return_counts=True)\n",
    "for value, count in zip(wine_customer_segment_unique, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos con las componentes principales. Primero estandarizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento. Estandarizar las variables\n",
    "# Iniciar scaler y aplicarlo\n",
    "sc = StandardScaler()\n",
    "wine_features_transformed = sc.fit_transform(wine_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué estandarizamos? El análisis es sensible a la varianza de las variables originales y eso puede ocasionar problemas a la hora de elegir los CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos PCA\n",
    "pca = PCA(n_components = 2)\n",
    "wine_scores = pca.fit_transform(wine_features_transformed) # Obtenemos los scores\n",
    "\n",
    "wine_scores[0:4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como vimos más arriba, esa transformación es equivalente a el producto matricial entre los datos y los loadings\n",
    "wine_scores_a_mano = np.matmul(wine_features_transformed, pca.components_.transpose())\n",
    "print(wine_scores_a_mano[0:4,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de la Varianza explicada por los componentes\n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)\n",
    "# El primer componente principal explica el 36% de la varianza, mientras que el segundo, explica el 19%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos features y loadings\n",
    "for i, loading_vector in enumerate(loading_vectors):\n",
    "    print(f\"\\nLoading Vector CP{i+1}:\")\n",
    "    for j, feature in enumerate(wine_data.columns[:-1]):\n",
    "        print(f\"{feature}: {round(loading_vector[j],3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame para los componentes principales\n",
    "pca_df = pd.DataFrame(data=wine_scores, columns=['Componente_1', 'Componente_2'])\n",
    "\n",
    "# Añadir la variable objetivo al DataFrame de los componentes principales\n",
    "pca_df['Customer_Segment'] = wine_customer_segment\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los componentes\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['Componente_1'], pca_df['Componente_2'], c=wine_data['Customer_Segment'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "plt.title('PCA - Components 1 and 2')\n",
    "plt.colorbar(label='Customer Segment')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Otra forma de graficar\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('Principal Component 1',fontsize=12)\n",
    "plt.ylabel('Principal Component 2',fontsize=12)\n",
    "plt.title(\"Wine Data\",fontsize=16)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "targets = [1, 2, 3]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indices_graf = pca_df['Customer_Segment'] == target\n",
    "    plt.scatter(pca_df.loc[indices_graf, 'Componente_1'], pca_df.loc[indices_graf, 'Componente_2'], c = color, s = 50)\n",
    "\n",
    "#plt.xlim(-4,4)\n",
    "#plt.ylim(-4,4)\n",
    "plt.legend(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación bidimensional de los datos tridimensionales capta correctamente el patrón principal de los datos: las observaciones rojas, azules y verdes, siguen estando en la representación bidimensional. \n",
    "\n",
    "Nota: aquí usamos dos componentes pero podríamos haber usado 1 o más de 2. Para decidir qué número de componentes usar, podemos consultar un scree plot que nos muestre la proporción de variable explicada para cada uno de los componentes y la variación en la varianza total explicada por el total de los componentes.\n",
    "\n",
    "Típicamente se elige la cantidad de componentes para la cual la proporción de la varianza explicada cae para cada componente principal adicional (cuando hay un codo en el scree plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ajustar una regresión logística con los componentes\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    " \n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(wine_scores, wine_customer_segment)\n",
    "\n",
    "# Ver coeficientes\n",
    "coefficients = classifier.coef_\n",
    "intercept = classifier.intercept_\n",
    "\n",
    "print(\"Coeficientes:\")\n",
    "for i, coef in enumerate(coefficients[0]):\n",
    "    print(f\"PC{i+1}: {coef}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Características organolépticas del café (entre otras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice los datos de los archivos `arabica_ratings_raw.csv` y `robusta_ratings_raw.csv` para realizar un análisis de las componentes principales de las evaluaciones organolépticas del café.\n",
    "\n",
    "Tendrá que combinar los datasets, seleccionar qué columnas son las importantes para el análisis y luego ajustar un PCA.\n",
    "\n",
    "Es interesante que observe cómo se representa cada variedad (Arabica y Robusta) en estas componentes principales. También cuál es la varianza explicada por las mismas.\n",
    "\n",
    "Siéntase libre de tomar las decisiones que considere necesarias (sin miedo al éxito) y concluya consecuentemente, los datos en el mundo real no vienen con instructivo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NFyrvNVL26HH",
    "pEiQnszy3lmi",
    "1yaJI88z4TfW",
    "ZoQVkU4C-GNd"
   ],
   "name": "Aprendizaje_no_supervisado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "988c801e8fa6188d3e53012a7256361dd6100dad47899d4700f624e035bcb20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
