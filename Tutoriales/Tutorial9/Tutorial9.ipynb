{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m3CnbNMV2Cp"
   },
   "source": [
    "# Tutorial de Big Data (UNT) 2024\n",
    "## Tutorial 9 - Cross-validation\n",
    "\n",
    "**Objetivo:** \n",
    "Que se familiaricen con la técnica de K-fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "BoSQOK1iV2Cs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from ISLP import load_data\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con la base 'Auto' de ISLP.\n",
    "\n",
    "Tiene información para 392 vehículos. Kilometraje de gasolina, caballos de fuerza El dataset tiene las siguiente variables:\n",
    "- mpg: millas por galón\n",
    "- cylinders: Número de cilindros entre 4 y 8\n",
    "- displacement: Cilindrada o desplazamiento del motor (pulgadas cúbicas)\n",
    "- horsepower: Caballos del motor\n",
    "- weight: Peso del vehículo (libras)\n",
    "- acceleration: Tiempo de aceleración de 0 a 100 km/h (seg.)\n",
    "- year: Año del modelo (módulo 100)\n",
    "- origin: Origen del vehículo (1. Americano, 2. Europeo, 3. Japonés)\n",
    "- name: Nombre del vehículo\n",
    "\n",
    "En este [link](https://islp.readthedocs.io/en/latest/datasets/Auto.html) tienen más información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv(\"Auto.csv\")\n",
    "\n",
    "# Dimensión de la base\n",
    "print(\"Dimensión del dataframe:\", auto.shape)\n",
    "\n",
    "# Variables e información\n",
    "#print(auto.dtypes)\n",
    "print(auto.info())\n",
    "\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay duplicados?\n",
    "print(\"Duplicados:\", auto.duplicated().sum())\n",
    "\n",
    "# Hay valores faltantes?\n",
    "print(\"\\n Missings:\\n\", auto.isnull().sum()) # conteo\n",
    "#print(auto.isnull().mean() * 100) # como porcentaje\n",
    "\n",
    "# No hay duplicados ni missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección rápida de las variables y sus valores\n",
    "auto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto[\"origin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notamos que origin es una variable categórica (toma valores 1, 2, 3)\n",
    "\n",
    "# Usaremos one-hot encoding para transformar la columna categórica llamada origin \n",
    "# en varias columnas binarias (dummies).\n",
    "# Cómo? get_dummies \n",
    "origin_dummies = pd.get_dummies(auto['origin'], prefix='origin')\n",
    "\n",
    "# Concatenamos con el df original\n",
    "auto_d = pd.concat([auto, origin_dummies], axis=1)\n",
    "auto_d.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a trabajar con mpg como variable dependiente y horsepower como independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "_fRHNPS6V2Ct"
   },
   "outputs": [],
   "source": [
    "# Guardo los vectores de variable dependiente y de variable independiente respectivamente:\n",
    "y = auto_d['mpg']\n",
    "X = auto_d['horsepower']\n",
    "X = np.array(X).reshape((-1, 1))\n",
    "\n",
    "# Parto la base en dos y transformo el vector x: \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión lineal\n",
    "lreg=LinearRegression()\n",
    "lreg.fit(x_train,y_train)\n",
    "y_pred_lreg=lreg.predict(x_test)\n",
    "\n",
    "print(\"R2:\", r2_score(y_test,y_pred_lreg))\n",
    "\n",
    "print(\"Coeficiente:\", lreg.coef_)\n",
    "y_pred_lreg = lreg.predict(x_test)\n",
    "ecm_lreg = mean_squared_error(y_test, y_pred_lreg)\n",
    "print('Error cuadrático medio (test):', ecm_lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roKZmBmrV2Cu"
   },
   "outputs": [],
   "source": [
    "# Recordemos las Regresiones Polinómicas:\n",
    "# Implican una transformación polinómica de las X, para luego implementar la regresión\n",
    "\n",
    "# Veamos un modelo cuadrático:\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False) \n",
    "# Recordar setear include_bias=False dado que en la regresión lineal -con LinearRegression- se incluirá la columna de 1s\n",
    "\n",
    "#print(x_train)\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.fit_transform(x_test)  \n",
    "np.set_printoptions(suppress = True) # evita que el print salga con notación científica\n",
    "print('X luego de la transformación:\\n', x_train_poly[:5,])\n",
    "\n",
    "# Ajustamos el modelo\n",
    "model = LinearRegression().fit(x_train_poly, y_train) \n",
    "print('\\nIntercepto:', model.intercept_)\n",
    "print('Coeficientes:', model.coef_)\n",
    "\n",
    "# Calculamos el Error Cuadrático Medio\n",
    "y_pred_poly = model.predict(x_test_poly)\n",
    "ecm2 = mean_squared_error(y_test, y_pred_poly)\n",
    "print('Error cuadrático medio (test):', ecm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGeVHCwAV2Cv"
   },
   "outputs": [],
   "source": [
    "# Veamos un modelo cúbico:\n",
    "poly = PolynomialFeatures(degree = 3, include_bias=False) \n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.fit_transform(x_test)  \n",
    "  \n",
    "model = LinearRegression().fit(x_train_poly, y_train) \n",
    "y_pred_poly = model.predict(x_test_poly)\n",
    "\n",
    "ecm3 = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print('\\nIntercepto:', model.intercept_)\n",
    "print('Coeficientes:', model.coef_)\n",
    "print('Error cuadrático medio (test):', ecm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, viendo el ECM, parecería que la regresión polinomial de grado 2 es la que mejor funciona "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0v8qlN3V2Cv"
   },
   "outputs": [],
   "source": [
    "# Creamos un nuevo vector de X y aplicamos las transformaciones\n",
    "X_seq = np.linspace(X.min(), X.max()).reshape(-1,1) \n",
    "# Valores entre el minimo y el maximo de X. \n",
    "# linspace por default crea 50 valores\n",
    "# Aplicamos las transformaciones polinomicas\n",
    "X_seq_poly = poly.fit_transform(X_seq)  \n",
    "\n",
    "# Gráfico\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(X_seq, model.predict(X_seq_poly),color=\"black\")\n",
    "plt.title(\"Polynomial regression with degree 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zMPx8mCV2Cx"
   },
   "outputs": [],
   "source": [
    "# Ahora supongamos que cambiamos la muestra y repetimos, hacemos lo mismo otra vez\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "# Qué error esperarían que obtengamos esta vez?\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False) \n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.fit_transform(x_test)  \n",
    "  \n",
    "model = LinearRegression().fit(x_train_poly, y_train) \n",
    "y_pred_poly = model.predict(x_test_poly)\n",
    "\n",
    "ecm3b = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print('\\nIntercepto:', model.intercept_)\n",
    "print('Coeficientes:', model.coef_)\n",
    "print('Error cuadrático medio (test):', ecm3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "Kvzk8UGzV2Cx"
   },
   "outputs": [],
   "source": [
    "# Cómo podemos repetir el código sin escribirlo por tercera vez?\n",
    "# Podemos hacer que nuestro código funcione para otros grados?\n",
    "\n",
    "def transf_reg_poly(grado, x_train, x_test, y_train, y_test):\n",
    "    '''\n",
    "    La función realiza una transformación polinomial y luego corre una regresión lineal polinómica\n",
    "    Input:\n",
    "        grado\n",
    "        x_train, x_test, y_train, y_test\n",
    "    Output:\n",
    "        modelo, ecm\n",
    "    '''\n",
    "    poly = PolynomialFeatures(degree = grado, include_bias=False) \n",
    "\n",
    "    x_train_poly = poly.fit_transform(x_train)\n",
    "    x_test_poly = poly.fit_transform(x_test)  \n",
    "  \n",
    "    model = LinearRegression().fit(x_train_poly, y_train) \n",
    "    y_pred_poly = model.predict(x_test_poly)\n",
    "    \n",
    "    ecm = mean_squared_error(y_test, y_pred_poly)\n",
    "    return model, ecm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ZbegRYh6V2Cy"
   },
   "outputs": [],
   "source": [
    "ecm1 = transf_reg_poly(1, x_train, x_test, y_train, y_test)[1]\n",
    "ecm2b = transf_reg_poly(2, x_train, x_test, y_train, y_test)[1]\n",
    "ecm3b = transf_reg_poly(3, x_train, x_test, y_train, y_test)[1]\n",
    "ecm4b = transf_reg_poly(4, x_train, x_test, y_train, y_test)[1]\n",
    "ecm5b = transf_reg_poly(5, x_train, x_test, y_train, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BjUojF5V2Cy"
   },
   "outputs": [],
   "source": [
    "print('Regresión lineal:', ecm_lreg)\n",
    "print('Grado2:', ecm2)\n",
    "print('Grado3:', ecm3)\n",
    "\n",
    "print('\\nRegresión lineal:', ecm1)\n",
    "print('Grado2:', ecm2b)\n",
    "print('Grado3:', ecm3b)\n",
    "print('Grado4:', ecm4b)\n",
    "print('Grado5:', ecm5b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que la regresión lineal funciona peor, y que introducir un término cuadrático reduce el ECM. Pero en el caso de introducir un término cúbico, no es obvio si funciona mejor o no...\n",
    "\n",
    "El ECM puede variar según qué observaciones quedaron incluidas en los sets de train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk05DUbrV2Cz"
   },
   "source": [
    "###  Nuevo enfoque: K-FOLD CROSS-VALIDATION  \n",
    "\n",
    "Es un **técnica de remuestreo**. Se usa para estimar el error (test) asociado a un método de aprendizaje, para:  \n",
    "- Elegir el nivel de complejidad optimo (Model selection)\n",
    "- Evaluar el error de pronóstico fuera de la muestra (futura, condicional, contra fáctica, etc.) (Model Assesment)\n",
    "\n",
    "Consiste en:\n",
    "- Dividir las observaciones en k folds (pliegues), del mismo tamaño, aleatoriamente. \n",
    "- Ajustar el modelo k veces, cada vez con k-1 folds (distintos cada vez). Computar k veces el error de predicción en el fold reservado. (cada fold se usa k-1 veces como training set y 1 vez como test set).\n",
    "- Estimar el error de predicción, estimación que surge de promediar las K estimaciones obtenidas.\n",
    "\n",
    "Vamos a usar [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) de Scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(url=\"https://global.discourse-cdn.com/dlai/optimized/3X/a/3/a3ed2de61c2b4fa00f1b7e939753e1a7e181afb0_2_690x476.png\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = auto['mpg']\n",
    "X = auto['horsepower']\n",
    "X = np.array(X).reshape((-1, 1))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "ecms = pd.DataFrame(columns=[\"grado\", \"particion\", \"ecm\"])\n",
    "ecms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo usual es usar K=5 o K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X), type(y))\n",
    "print(X.shape, y.shape)\n",
    "#print(X.flatten(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktqljY-fV2Cz"
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "for grado in range(2, 10):   \n",
    "\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=100)\n",
    "    \n",
    "    # El método kf.split aplicado a X nos da los conjuntos de índices que necesitamos para\n",
    "    # partir nuestros conjunto de datos en training y testing en cada iteración.\n",
    "    #  OXXXX\n",
    "    #  XOXXX\n",
    "    #  XXOXX\n",
    "    #  XXXOX\n",
    "    #  XXXXO\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X)):   \n",
    "        x_train, x_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        #print(i, x_train.shape[0])\n",
    "        \n",
    "        ecm = transf_reg_poly(grado, x_train, x_val, y_train, y_val)[1]\n",
    "            \n",
    "        df_i = pd.DataFrame({\"grado\": grado, \"particion\": i, \"ecm\": ecm}, index=[0])\n",
    "        ecms = pd.concat([ecms, df_i])\n",
    "    \n",
    "ecms = ecms.astype({\"grado\":int, \"particion\":int})\n",
    "ecms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una corrección para no contaminar los datos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "print(type(X_train), type(y_train))\n",
    "print(X_train.shape, y_train.shape)\n",
    "#print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktqljY-fV2Cz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecms = pd.DataFrame(columns=[\"grado\", \"particion\", \"ecm\"])\n",
    "ecms\n",
    "\n",
    "K = 5\n",
    "\n",
    "for grado in range(1, 10):   \n",
    "\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=100)\n",
    "    \n",
    "    # El método kf.split aplicado a X nos da los conjuntos de índices que necesitamos para\n",
    "    # partir nuestros conjunto de datos en training y testing en cada iteración.\n",
    "    #  OXXXX\n",
    "    #  XOXXX\n",
    "    #  XXOXX\n",
    "    #  XXXOX\n",
    "    #  XXXXO\n",
    "    \n",
    "    for i, (train_index2, val_index2) in enumerate(kf.split(X_train)):\n",
    "        X_train_fold, X_val_fold = X_train[train_index2], X_train[val_index2]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index2], y_train.iloc[val_index2]\n",
    "        #print(i, X_train_fold.shape[0])\n",
    "        \n",
    "        ecm = transf_reg_poly(grado, X_train_fold, X_val_fold, y_train_fold, y_val_fold)[1]\n",
    "            \n",
    "        df_i = pd.DataFrame({\"grado\": grado, \"particion\": i, \"ecm\": ecm}, index=[0])\n",
    "        ecms = pd.concat([ecms, df_i])\n",
    "    \n",
    "ecms = ecms.astype({\"grado\":int, \"particion\":int})\n",
    "ecms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cómo elegir el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThaTPH93V2C0"
   },
   "outputs": [],
   "source": [
    "# Una opción: visualizar los ECMs en un boxplot\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "ss = sns.boxplot(data=ecms, x=\"grado\", y=\"ecm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJJix54xV2C0"
   },
   "outputs": [],
   "source": [
    "# Una opción para ver el mejor modelo sería sacar el error promedio para cada grado:\n",
    "ecms_avg = ecms.groupby('grado').agg({'ecm':'mean'})\n",
    "ecms_avg.reset_index(inplace = True)\n",
    "ecms_avg.astype({\"grado\":int})\n",
    "ecms_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr5xl6PxV2C0"
   },
   "outputs": [],
   "source": [
    "# Función para seleccionar \n",
    "min_ecm = np.Inf\n",
    "grado = None\n",
    "\n",
    "for index, row in ecms_avg.iterrows():\n",
    "    if row['ecm'] < min_ecm:\n",
    "        min_ecm = row['ecm']\n",
    "        grado = row['grado'].astype(int)\n",
    "\n",
    "print('El mínimo error es', round(min_ecm, 2), 'y se da con un polinomio de grado', grado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O14qkQTSV2C0"
   },
   "outputs": [],
   "source": [
    "# Finalmente construimos el modelo polinomial de grado 2 y lo graficamos \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "modelo = transf_reg_poly(grado, x_train, x_test, y_train, y_test)[0]\n",
    "ecm = transf_reg_poly(grado, x_train, x_test, y_train, y_test)[1]\n",
    "        \n",
    "X_seq = np.linspace(X.min(), X.max()).reshape(-1,1)\n",
    "poly = PolynomialFeatures(degree = grado, include_bias=False) \n",
    "X_seq_poly = poly.fit_transform(X_seq)  \n",
    "\n",
    "x_test_poly = poly.fit_transform(x_test)  \n",
    "y_pred_poly = modelo.predict(x_test_poly)\n",
    "ecm_final = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(X_seq, modelo.predict(X_seq_poly),color=\"black\")\n",
    "plt.title(\"Polynomial regression with degree {}\".format(grado))\n",
    "plt.show()\n",
    "\n",
    "ecm_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial 8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CC408",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
