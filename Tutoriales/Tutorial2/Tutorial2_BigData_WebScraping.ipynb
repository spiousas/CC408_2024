{"cells":[{"cell_type":"markdown","metadata":{"id":"8ekyFN3KHriJ"},"source":["# Tutoriales de Ciencia de Datos (CC408-2024)\n","## Tutorial 2a\n","\n","El objetivo de esta clase es ver cómo extraer datos de internet por medio de Web Scraping y cómo interactuar con una APIs. También veremos un pequeño ejemplo de _sentiment analysis_.\n","\n","- Web Scraping\n","- APIs\n","- Sentiment analysis -si llegamos, sino les queda como tarea mirarlo-\n"]},{"cell_type":"markdown","metadata":{"id":"Evl3Po2pHriN"},"source":["### Web Scraping: extrayendo datos de internet"]},{"cell_type":"markdown","metadata":{"id":"uRDvgeBU7IOx"},"source":["#### ¿Qué es web scraping?\n","\n","La práctica de recopilar datos a través de cualquier medio que no sea un programa que interactúa con una API o un humano que usa un navegador web. En general esto se logra mediante un programa automatizado que consulta un servidor web, solicita datos (generalmente en forma de HTML y otros archivos que componen las páginas web) y luego analiza esos datos para extraer la información necesaria.\n","\n","<font color=\"gray\">\n","Fuente: Ryan Mitchell (2015). Web Scraping with Python.\n","<font>\n"]},{"cell_type":"markdown","metadata":{"id":"0wYrC2Tb7IOx"},"source":["#### Antes de empezar ⚠️\n","\n","#### Aspectos éticos y legales del web scraping\n","Web scraping es la extracción de datos de sitios web, es una forma automática de guardar información que se presenta en nuestro navegador (muy usada tanto en la industria como en la academia). Sus aspectos legales dependerán de cada sitio. Respecto a la ética es importante que nos detengamos a pensar si estamos o no generando algun perjuicio.\n","\n","#### No reinventar la rueda\n","Emprender un proyecto de web scraping a veces es rápido y sencillo, pero en general requiere tiempo y esfuerzo. Siempre es aconsejable asegurarse de que valga la pena y antes iniciar hacerse algunas preguntas:\n","* ¿La informacion que necesito ya se encuentra disponible? (ej: APIs)\n","* ¿Vale la pena automatizarlo o es algo que lleva poco trabajo a mano?"]},{"cell_type":"markdown","metadata":{"id":"fSSgMvEb7IOx"},"source":["#### Conceptos básicos sobre la web\n","\n","HTML, CSS y JavaScript son los tres lenguajes principales con los que está hecho la parte de la web que vemos (*front-end*).\n","\n","Una analogía para entender cómo funcionan:\n","- HTML como la estructura de la casa.\n","- CSS como la decoración interior y exterior.\n","- JavaScript como el sistema eléctrico, del agua y otras funcionalidades que hacen una casa habitable\n","\n","<center>\n","<img src=\"https://www.nicepng.com/png/detail/142-1423886_html5-css3-js-html-css-javascript.png\" width=\"400\">\n","\n","<img src=\"https://geekflare.com/wp-content/uploads/2019/12/css-gif.gif\" width=\"243\">\n","\n","\n","</center>\n","\n","<br>\n","<br>\n","\n","| ESTRUCTURA  | ESTILO | FUNCIONALIDAD|\n","|-----|----------------| ---------- |\n","|HTML| CSS | JAVASCRIPT|\n","\n","Si quieren ver más cómo se unen HTML+CSS+Javascript: https://codepen.io/voubina/pen/gOZGPYx\n","\n","\n","<font color=\"gray\">    \n","Fuente de las imágenes: <br>\n","https://geekflare.com/es/css-formatting-optimization-tools/ <br>\n","https://www.nicepng.com/ourpic/u2q8i1o0e6q8r5t4_html5-css3-js-html-css-javascript/\n","\n","Fuente de la información: Instituto Humai - Curso de Automatización\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"W5L-SSyY7IOy"},"source":["##### HTML\n","\n","- HTML quiere decir: lenguaje de marcado de hipertexto o HyperText Markup Language por sus siglas en inglés.\n","- El código  HTML da estructura a los sitios web.\n","- El código HTML se conforma por distintos elementos que le dicen al navegador cómo mostrar el contenido.\n","- Esos elementos son etiquetas. Hay etiquetas para indicar qué contenido es un título, un párrafo, un enlace, una imagen, etc.\n","\n","|Etiqueta (Tag)     |Descripción|\n","|:--------|:--------|\n","|`<!DOCTYPE>`  | \tDefine el tipo de documento|\n","|`<html>`      |\tDefine un documento HTML |\n","|`<head>`      |\tContiene metadata/información del documento|\n","|`<title>`     |\tDefine el títutlo del documento|\n","|`<body>`      |\tDefine el cuerpo del documento|\n","|`<h1>` a `<h6>`|\tDefine títulos |\n","|`<p>`         |\tDefine un párrafo|\n","|`<br>`        |\tInserta un salto de línea (line break) |\n","|`<!--...-->`\t |  Define un comentario|\n","    \n","Para saber más sobre HTML podés consultar [acá](https://www.w3schools.com/TAGS/ref_byfunc.asp) la lista de etiquetas de este lenguaje."]},{"cell_type":"markdown","metadata":{"id":"iyQQ388O7IOy"},"source":["#### ¿Cómo consigo el código HTML?\n","\n","Ahora que sabemos cuál es el componente principal de los sitios webs podemos intentar programar a nuestra computadora para leer HTML y extraer información útil.\n","\n","Para conseguir el código de un sitio web podemos:\n","- Ir a herramientas del desarrollador (`ctrl+shift+i`) en el navegador.\n","- Presionar `ctrl+u` en el navegador.\n","\n","Para hacer lo mismo desde Python podemos usar la librería requests (vamos a verlo ahora)"]},{"cell_type":"markdown","metadata":{"id":"tiSLCPpG7IOy"},"source":["### Primer ejemplo: títulos de noticias"]},{"cell_type":"markdown","metadata":{"id":"bKceEPmG7IOy"},"source":["#### **Método: BeautifulSoup**\n","* Esta librería provee un *parser* de html, o sea un programa que analiza/entiende el código. Así, nos permite hacer consultas más sofisticadas de forma simple, por ejemplo: \"buscar todos los títulos h2 del sitio\".\n","\n","* Se usa para extraer los datos de archivos HTML. Crea un árbol de análisis a partir del código fuente de la página que se puede utilizar para extraer datos de forma jerárquica y más legible.\n","\n","<center>\n","<img alt=\"\" width=\"700\" role=\"presentation\" src=\"https://miro.medium.com/max/700/0*ETFzXPCNHkPpqNv_.png\"> <br>\n","\n","<font color=\"gray\">\n","Fuente de la información: Instituto Humai - Curso de Automatización\n","<font>\n"]},{"cell_type":"markdown","metadata":{"id":"Ax58or5J7IOy"},"source":["Empecemos!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1729,"status":"ok","timestamp":1724075814448,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"LKlBIroEHriO"},"outputs":[],"source":["#!pip install requests\n","#!pip install BeautifulSoup\n","#!pip install pandas\n","# Nota: si no tienen instaladas las librarías a importar debajo, primero deben instalarlas\n","# (para eso, quiten el # y activen las 3 líneas de código de arriba)\n","\n","import requests #html requestor\n","from bs4 import BeautifulSoup #html parser\n","import pandas as pd #dataframe manipulator\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3153,"status":"ok","timestamp":1724075825937,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"weyX3ZRM7IOz","outputId":"e529e681-150c-43bb-fdec-fd5cb4746981"},"outputs":[],"source":["url = \"https://www.lanacion.com.ar/\"\n","\n","r = requests.get(url) #traigo el contenido del html\n","contenido = r.content\n","\n","contenido"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["soup = BeautifulSoup(contenido, \"html.parser\")\n","soup"]},{"cell_type":"markdown","metadata":{"id":"Qkkvn4CA7IOz"},"source":["Seleccionando un título que aparece bajo la etiqueta o tag h2, vemos, por ejemplo:"]},{"cell_type":"markdown","metadata":{},"source":["### El fiscal Ramiro González había pedido que le enviaran lo grabado dentro de la quinta presidencial, tanto en la residencia oficial como en la casa de huéspedes, pero lo registrado no se guarda más de 45 días"]},{"cell_type":"markdown","metadata":{},"source":["Nota: esto cambia según el día en que hagan el request o pedido (ya que la página de noticias se actualiza)"]},{"cell_type":"markdown","metadata":{"id":"LLJrSep-7IOz"},"source":["### Opción A - Usando find y find_all"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1724075840849,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"P3QvxXKh7IOz","outputId":"0d54b732-6a32-4573-f013-bc900d10c176","scrolled":false},"outputs":[],"source":["# Dentro de la sopa, busco los elementos que contienen la información que necesito\n","# Buscamos el elemento <h2> indicando la clase (class). Escribo class_ porque \"class\" es una palabra reservada en Python\n","h2_element = soup.find('h2')\n","print(h2_element)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Obtenemos el texto del elemento <h2>\n","h2_text = h2_element.text.strip()  # strip() permite remover espacios sobrantes\n","print('\\n', h2_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1724075846911,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"PWgbH_Qh7IOz","outputId":"a6d7c959-3962-49c0-ed59-2b03e28d845f"},"outputs":[],"source":["# Obtuvimos el *primer* elemento de la página con ese tag. Pero queremos hacerlo para todos los títulos...\n","# El método \"find_all\" busca TODOS los elementos de la página con ese tag y devuelve una lista que los contiene\n","# (en realidad devuelve un objeto de la clase \"bs4.element.ResultSet\")\n","h2_elements = soup.find_all('h2')\n","\n","print(type(h2_elements))\n","print('\\n', h2_elements)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1724075850995,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"LIpFt_Be7IOz","outputId":"cc111699-7370-4dcf-df86-6b690e3aa7d2"},"outputs":[],"source":["# Extraemos el texto de cada elemento <h2> e imprimimos\n","for h2_element in h2_elements:\n","    h2_text = h2_element.text.strip()\n","    print(h2_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1724075855798,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"m6sr_N6g7IOz"},"outputs":[],"source":["# Idealmente, tenemos que guardar estos títulos, queremos analizarlos\n","titulos = [] # primero creamos una lista\n","\n","# Extraemos el texto de cada elemento <h2> y ahora guardamos\n","for h2_element in h2_elements:\n","    h2_text = h2_element.text.strip()\n","    #print(h2_text)\n","\n","    titulos.append({\n","        'titular': h2_text\n","    })\n","\n","# Creamos un dataframe a partir de la lista de títulos\n","titulos_df = pd.DataFrame(titulos)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1724075857965,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"Y8ihxSQl7IOz","outputId":"bab46262-37a7-42ff-bf18-5f8999b766c9"},"outputs":[],"source":["titulos_df"]},{"cell_type":"markdown","metadata":{"id":"HbyuEwhN7IO0"},"source":["### Análisis de sentimiento de los títulos de noticias\n","\n","Más información sobre sentiment analysis, acá: https://www.datacamp.com/tutorial/text-analytics-beginners-nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5504,"status":"ok","timestamp":1724075912698,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"_4kRlxLF7IO0","outputId":"9e81a03d-6f52-4559-9eba-3c86d5ad353b"},"outputs":[],"source":["# Si aún no instalaron estas librerías, activar estas líneas de código -quitar #- para instalarlas\n","#!pip install string\n","#!pip install pandas\n","#!pip install nltk\n","#!pip install stop-words\n","#!pip install spacy\n","#!python -m spacy download es_core_news_sm\n","#!pip uninstall vaderSentiment\n","#!pip install vader-multi"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10179,"status":"ok","timestamp":1724075924438,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"-Uk4qwb47IO0"},"outputs":[],"source":["# Importamos los paquetes a utilizar\n","import string\n","import pandas as pd\n","import nltk # para procesamiento del lenguaje natural\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from stop_words import get_stop_words\n","from nltk.stem import WordNetLemmatizer\n","import spacy # para preprocesamiento en español\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# ntlk requiere descargar algunos datos adicionales\n","# nltk.download('all')\n","\n","# Para trabajar en inglés usar:\n","#from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from stop_words import get_stop_words"]},{"cell_type":"markdown","metadata":{"id":"KfWzSh8K7IO0"},"source":["Vamos a limpiar los títulos\n","Como parte del preprocesamiento de la información tenemos los siguientes pasos:\n","1. Tokenization: Implica dividir el texto en palabras (o tokens)\n","2. Eliminar stop words: quitar palabras comunes e irrelevantes que no tienen mucho \"sentimiento\". Esto permite mejorar la precisión del análisis de sentimiento\n","3. Lemmatization: reducir las palabras a sus raíces (por ejemplo, eliminando sufijos, pasar de \"leyendo\" a \"leer\")."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1724075931750,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"iRurB_UU7IO0","outputId":"fbfabb52-3032-4241-c18e-84ebaf4ed786","scrolled":true},"outputs":[],"source":["# Veamos la lista de signos de puntuación\n","print(string.punctuation)\n","# Como estamos trabajando en español, es conveniente agregar algunos símbolos más a los signos de puntuación\n","string.punctuation = string.punctuation + '¿¡“”'\n","print(string.punctuation)\n","\n","# Cargar palabras vacías en español\n","stop_words = get_stop_words('spanish')\n","print(stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1724075936727,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"5eQroZGm7IO0"},"outputs":[],"source":["def limpiar_titulos_nltk(titulo):\n","    '''\n","    Esta función limpia el texto del título.\n","    Convierte texto en tokens, elimina stop words, y transforma palabras en su forma raíz\n","    para dejar en el texto solo las palabras con mayor contenido.\n","    Input:\n","        título (str): Texto del título original\n","    Output:\n","        título (str): Texto del título limpio\n","    '''\n","\n","    # 1. Separar los títulos en tokens (obtenemos una lista con palabras)\n","    word_tokens = word_tokenize(titulo.lower())\n","\n","    # 2. Eliminar palabras vacías (stop words) de los títulos\n","    # Loop por las condiciones\n","    filtered_tokens = []\n","    for w in word_tokens:\n","        # Verificamos tokens contra stop words y puntuación\n","        if w not in stop_words and w not in string.punctuation:\n","            filtered_tokens.append(w)\n","\n","    # 3. Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","\n","    lemmatized_tokens = []\n","    for w in filtered_tokens:\n","        lemmatizer.lemmatize(w)\n","        lemmatized_tokens.append(w)\n","\n","    # Volvemos a armar la oración (concatenamos las palabras separándolas con un espacio)\n","    return ' '.join(lemmatized_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":288,"status":"error","timestamp":1724075940525,"user":{"displayName":"Ignacio Spiousas","userId":"14960225508792812313"},"user_tz":180},"id":"j6BrhQo27IO0","outputId":"92b26f4a-160f-42d1-8821-46f4cc0dbc27"},"outputs":[],"source":["# Cargar el modelo para el español y las stop words según spacy\n","nlp = spacy.load('es_core_news_sm')\n","stopwords_spacy = spacy.lang.es.stop_words.STOP_WORDS\n","\n","def limpiar_titulos_spacy(titulo):\n","    '''\n","    Esta función limpia el texto del título (usando funcionalidades de la librería spacy).\n","    Convierte texto en tokens, elimina stop words, y transforma palabras en su forma raíz\n","    para dejar en el texto solo las palabras con mayor contenido.\n","    Input:\n","        título (str): Texto del título original\n","    Output:\n","        título (str): Texto del título limpio\n","    '''\n","\n","    # Procesar el texto con spaCy\n","    doc = nlp(titulo.lower())\n","    #print(doc)\n","\n","    filtered_tokens = []\n","    lemmas = []\n","\n","    # Pasar a tokens y eliminar puntación y stopwords\n","    for w in doc:\n","        if w.text not in stopwords_spacy and not w.is_punct:\n","            filtered_tokens.append(w.text)\n","    filtered_doc = ' '.join(filtered_tokens)\n","\n","    # Obtener las formas lematizadas de las palabras\n","    doc2 = nlp(filtered_doc)\n","    for w_f in doc2:\n","        lemmas.append(w_f.lemma_)\n","\n","    # Volvemos a armar la oración (concatenamos las palabras separándolas con un espacio)\n","    return ' '.join(lemmas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MnjiBft7IO0","outputId":"3fe77b4c-f35d-4e19-fdca-d939c2b26d8b"},"outputs":[],"source":["#Este es un título sucio:\n","titulos[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcfZCRvZ7IO0","outputId":"8b2df584-8140-4630-b808-e19f0bee4e88"},"outputs":[],"source":["#Este es un título limpio:\n","limpiar_titulos_nltk(titulos[0]['titular'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgBARBub7IO0","outputId":"eda092eb-69f1-4822-d7a8-9fffd9775242"},"outputs":[],"source":["#Este es un título limpio:\n","limpiar_titulos_spacy(titulos[0]['titular'])"]},{"cell_type":"markdown","metadata":{"id":"y3L2PdEE7IO0"},"source":["#### Ahora vamos a usar sentiment analysis para ver qué tan positivos son los títulos\n","\n","Vamos a usar la bilioteca NLTK (Natural Language Toolkit) para clasificar los títulos en positivos o negativos. NLTK es una biblioteca de Python muy utilizada en procesamiento de lenguaje natural (NLP). VADER (Valence Aware Dictionary and Sentiment Reasoner) es un módulo específico de NLTK que se utiliza para el análisis de sentimientos.\n","\n","VADER es una herramienta especialmente diseñada para el análisis de sentimientos en textos. A diferencia de algunos enfoques más generales que utilizan modelos de aprendizaje automático, VADER se basa en un conjunto de reglas y un diccionario que asigna puntuaciones de polaridad a palabras y expresiones. Además, tiene en cuenta factores como las mayúsculas, los signos de puntuación y los emoticonos para evaluar la intensidad del sentimiento.\n","\n","Las puntuaciones de VADER incluyen la polaridad (positiva, negativa o neutra) y una medida de la intensidad del sentimiento. Es especialmente útil para textos informales o con lenguaje coloquial, como se encuentra comúnmente en redes sociales."]},{"cell_type":"markdown","metadata":{"id":"sxOOq1YG7IO0"},"source":["Vamos a usar un módulo de VADER \"multi\", que resuelve la misma tarea pero con otro modelo. Ver: https://github.com/brunneis/vader-multi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuJWe0yO7IO4","outputId":"383cc28d-86eb-495c-a68c-718362536f6c"},"outputs":[],"source":["# También pueden usar VADER \"original\" (Ver: https://www.nltk.org/_modules/nltk/sentiment/vader.html).\n","# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SentimentIntensityAnalyzer\n","\n","# Inicializar el analizador de sentimientos VADER\n","sia = SentimentIntensityAnalyzer()\n","\n","# Primero veamos un ejemplo\n","texto_ej_pos = \"Me encanta este curso\"\n","texto_ej_neg = \"Odio este curso\"\n","texto_ej_neu = \"Este curso me da igual\"\n","\n","print(texto_ej_pos, sia.polarity_scores(texto_ej_pos))\n","print(texto_ej_neg, sia.polarity_scores(texto_ej_neg))\n","print(texto_ej_neu, sia.polarity_scores(texto_ej_neu))\n","# Si la variable compound es positiva, el texto es positivo; si es negativa, el texto es negativo\n","# Y si se encuentra en el rango del 0 es un mensaje neutro"]},{"cell_type":"markdown","metadata":{"id":"Y8LE35n37IO4"},"source":["Ahora crearemos funciones que, además de dar un valor, clasifiquen en positivo, negativo o neutro"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D64L98gM7IO4"},"outputs":[],"source":["def analizar_sentiment(text):\n","    # Obtener la polaridad del sentimiento\n","    sia = SentimentIntensityAnalyzer()\n","    sentiment_score = sia.polarity_scores(text)\n","\n","    # Clasificar el sentimiento como positivo, negativo o neutro\n","    compound_score = sentiment_score['compound']\n","    if compound_score >= 0.05:\n","        sentiment = \"Positivo\"\n","    elif compound_score <= -0.05:\n","        sentiment = \"Negativo\"\n","    else:\n","        sentiment = \"Neutro\"\n","\n","    return compound_score, sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO9GsdI37IO4","outputId":"ec048c49-9157-4b6c-ae72-8062caa0c5ff"},"outputs":[],"source":["# Ejemplos de uso\n","compound_score_pos, sentiment_pos = analizar_sentiment(texto_ej_pos)\n","print(f\"Texto: {texto_ej_pos}\")\n","print(f\"Puntuación de sentimiento compuesta: {compound_score_pos}\")\n","print(f\"Sentimiento: {sentiment_pos}\")\n","\n","compound_score_neg, sentiment_neg = analizar_sentiment(texto_ej_neg)\n","print(f\"\\nTexto: {texto_ej_neg}\")\n","print(f\"Puntuación de sentimiento compuesta: {compound_score_neg}\")\n","print(f\"Sentimiento: {sentiment_neg}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzSbN7pA7IO4","outputId":"271800cd-4407-4567-e1ef-c032fb8a5dd2"},"outputs":[],"source":["# Ahora un ejemplo con un título limpio\n","print(titulos[0]['titular'],\n","      \"\\n\",\n","      analizar_sentiment(limpiar_titulos_nltk(titulos[0]['titular'])))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["titulos[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUVCQA8M7IO4"},"outputs":[],"source":["# Aplicamos la función para limpiar títulos para tener un columna con títulos limpios\n","titulos_df['titular_limpio'] = titulos_df['titular'].apply(limpiar_titulos_nltk)\n","# Vemos el sentiment\n","titulos_df['sentiment'] = titulos_df['titular_limpio'].apply(analizar_sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A02-MaJQ7IO4","outputId":"42dc0edf-c865-4406-f928-a5dad97d481e"},"outputs":[],"source":["titulos_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Separo en dos columnas\n","titulos_df[['value', 'emotion']] = pd.DataFrame(titulos_df['sentiment'].tolist(), index=titulos_df.index)\n","titulos_df_final = titulos_df.drop('sentiment', axis=1)\n","titulos_df_final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XX5P8LcD7IO4","outputId":"692c346c-764c-407e-a04d-dd94c588d042"},"outputs":[],"source":["# Podemos ver cuántos títulos con cada tipo de emoción clasificamos\n","titulos_df_final['emotion'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuPVCDcM7IO4"},"outputs":[],"source":["import openpyxl\n","# Y los podemos guardar como excel\n","titulos_df_final.to_excel('titulos.xlsx', index=False)"]},{"cell_type":"markdown","metadata":{"id":"8DWV4H_07IO4"},"source":["#### Un ejemplo en inglés usando TextBlob\n","\n","Para que les quede de referencia por si quieren hacer algo en inglés en el TP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69pY_E0q7IO4"},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHEeP9ph7IO4"},"outputs":[],"source":["# Ejemplo de texto en inglés\n","texto_ej_1 = \"I love learning about Big data\"\n","texto_ej_2 = \"I hate learning about Big data\"\n","\n","# Crear un objeto TextBlob con el texto\n","blob1 = TextBlob(texto_ej_1)\n","\n","# Obtener la polaridad del sentimiento (-1 a 1)\n","polarity1 = blob1.sentiment.polarity\n","\n","# Clasificar el sentimiento como positivo, negativo o neutro\n","def clasif_polarity(polarity):\n","    if polarity > 0:\n","        sentiment = \"Positivo\"\n","    elif polarity < 0:\n","        sentiment = \"Negativo\"\n","    else:\n","        sentiment = \"Neutro\"\n","    return sentiment\n","\n","# Mostrar los resultados\n","print(f\"Texto: {texto_ej_1}\")\n","print(f\"Polaridad del sentimiento: {polarity1}\")\n","print(f\"Sentimiento: {clasif_polarity(polarity1)}\")"]},{"cell_type":"markdown","metadata":{"id":"30Pp_A5e7IO5"},"source":["### Otro Ejemplo de Web Scraping: Tabla"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a _scrappear_ una tabla de episodios de Game of Thrones de la lista de episodios que aparece en Wikipedia (https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes). Les recomiendo que entren al link para ver más o menos la estructura de lo que queremos _scrappear_. Lo que vamos a tratar de obtener es una lista con todos los episodios, su fecha de emisión, director, escritor y cantidad de personas que lo vierons.\n","\n","Para eso vamos a utilizar una función de pandas que nos permite leer html: `read_html`\n","\n","Estén seguros de tener instalado el módulo `lxml`. Lo pueden instalar con `conda install lxml`, con `pip install lxml` o con el instalador de módulos gráfico del Anaconda Navigator.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ilt1X2jHriP"},"outputs":[],"source":["url = 'https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes'"]},{"cell_type":"markdown","metadata":{},"source":["Ahora que tenemos la dirección en la variable `html` vamos a usar la función `read_html`:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","table = pd.read_html(url)\n","print(len(table))"]},{"cell_type":"markdown","metadata":{},"source":["Vemos que table tiene 20 elementos. Veamos el elemento 0."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_seasons = table[0]\n","df_seasons"]},{"cell_type":"markdown","metadata":{},"source":["Se ve que es la primera tabla que aparece en la página web, una tabla que contiene la cantidad de episodios por temporada. Entonces busquemos la primera y la última de las tablas que nos interesan."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(table[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(table[8])"]},{"cell_type":"markdown","metadata":{},"source":["Después ya podemos ver que las tablas siguientes con de cosas que no nos interesan."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(table[9])"]},{"cell_type":"markdown","metadata":{},"source":["Armemos un subset con las tablas que nos interesan:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["subset = table[1:9]\n","print(subset)"]},{"cell_type":"markdown","metadata":{},"source":["Ahora podemos directamente concatenar todas las tablas (una de las ventajas de usar el _parser_ de `pandas` es que las tablas ya son dataframes)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_total = pd.concat(subset)\n","display(df_total)"]},{"cell_type":"markdown","metadata":{},"source":["Pero qué pasa si queremos agregar una columna que contenga el número de temporada."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(len(subset)):\n","    subset[i]['season'] = i + 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_total = pd.concat(subset)\n","display(df_total)"]},{"cell_type":"markdown","metadata":{},"source":["Y por último, movamos la columna season al principio:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["columns = ['season'] + [col for col in df_total.columns if col != 'season']\n","print(columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_total = df_total[columns]\n","df_total.tail()"]},{"cell_type":"markdown","metadata":{"id":"2qj9Pr1THriU"},"source":["Listo, ahora los podemos exportar como archivo `csv`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Knj6jgwQHriU"},"outputs":[],"source":["df_total.to_csv('GoT_episode_list.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Unos ejercicios para practicar\n","\n","##### 1 - Web scrapping de una tabla y sentiment analysis\n","\n","Obtenga los títulos de los episodios de Friends y ordenarlos por la polaridad. \n","\n","- ¿Cuántos títulos clasificados como positivos y negativos hay? ¿Por qué cree que esto es así?\n","- ¿Cuál es la temporada con el promedio de valencia más alto? ¿Y la que tiene el más bajo?\n","\n","_Pistas_:\n","- Los títulos de los episodios de Friends están en esta [URL](https://es.wikipedia.org/wiki/Anexo:Episodios_de_Friends).\n","- Pueden reciclar la función de limpiar títulos con `spacy` (`limpiar_titulos_spacy`) pero setenado `nlp = spacy.load('en_core_web_sm')` (usar el modelo de tokenización y lematización en inglés) y `stopwords_spacy = spacy.lang.en.stop_words.STOP_WORDS` (usar las stop words del inglés). Puede que necesiten correr `python -m spacy download en_core_web_sm` en la terminal o en el Anaconda prompt para bajar el modelo en inglés.\n","- Pueden usar la misma función que creamos con VADER para analizar el sentimiento una vez que los títulos ya están limpios (`analizar_sentiment`).\n","\n","##### 2 - Web scrapping de una página\n","\n","Obtenga los titulares del Diario El País de España."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
